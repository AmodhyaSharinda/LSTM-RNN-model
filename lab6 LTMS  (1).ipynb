{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "189100ed-a1bb-4ffb-b87e-de0f9a7eeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#from keras.utils import np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a45ccdd-a8d0-4b54-85b8-9db156137f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data and turn to lowercase\n",
    "filename = 'data.txt'\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text =raw_text.lower() #reduce no of unique characcters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26e1d7a3-e3b8-43a9-9ad3-40e049aeb764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"project gutenberg's alice's adventures in wonderland, by lewis carroll\\n\\nthis ebook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.  you may copy it, give it away or\\nre-use it under the terms of the project gutenberg license included\\nwith this ebook or online \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read first 300 characters\n",
    "raw_text[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b16a8ae8-a5e2-4a8a-9c07-204c31002e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now these characters cant unders stand bn nural net we hv to conver characters to numbers\n",
    "\n",
    "#1st find unique charcters in text ==>  we can use set to find unique charcterss it remove duplicates when creating a set\n",
    "#then put set of charcters to list and sorted in order\n",
    "\n",
    "char  = sorted(list(set(raw_text)))\n",
    "\n",
    "#now map charcter to integer\n",
    "#enmurate ==> give number with character \n",
    "#0 h\n",
    "#1 e\n",
    "#2 l\n",
    "#3 l\n",
    "#4 o\n",
    "\n",
    "#using dict we pair them\n",
    "\n",
    "chars_to_int = dict((c, i) for i, c in enumerate(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0d003a4-a48f-42be-b360-8ac5ebbfd6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '%': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '?': 27, '@': 28, '[': 29, ']': 30, '_': 31, 'a': 32, 'b': 33, 'c': 34, 'd': 35, 'e': 36, 'f': 37, 'g': 38, 'h': 39, 'i': 40, 'j': 41, 'k': 42, 'l': 43, 'm': 44, 'n': 45, 'o': 46, 'p': 47, 'q': 48, 'r': 49, 's': 50, 't': 51, 'u': 52, 'v': 53, 'w': 54, 'x': 55, 'y': 56, 'z': 57}\n"
     ]
    }
   ],
   "source": [
    "#character in the text withing dict\n",
    "print(chars_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfef5837-9057-4984-b7d6-e49522625f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total charcters :  163780\n"
     ]
    }
   ],
   "source": [
    "#no of charcters in text file\n",
    "n_chars = len(raw_text)\n",
    "print(\"total charcters : \" , n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a03b81b2-2bae-401e-a408-34c8cd1a17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total charcters :  58\n"
     ]
    }
   ],
   "source": [
    "#no of unique charcters in text file(vocabulary)\n",
    "n_vocab = len(char)\n",
    "print(\"total charcters : \" , n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa6151ff-ba5b-41a5-983f-d7039fe8c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the dataset of input to output pairs encoded as integers.\n",
    "#now we need x and y data to train and test model\n",
    "#so we take range of charcater as x and nexr charcater as y\n",
    "#chapt --> e  \n",
    "#hapte --> r\n",
    "\n",
    "#next we take the integer related to charcter usin char_to_int dictionary and put it into varible lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c87bf25-602f-40fc-908d-6902b5430f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns :  163680\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100 #range of data we take as x in sequentialy\n",
    "dataX = [] #independet(x) text pattern\n",
    "dataY = [] #next charcter of the pattern\n",
    "\n",
    "#loop to create X & Y and put to lists\n",
    "#n_chars - seq_length ==> use this bucuse at the end of last 100 charcters text is over\n",
    "for i in range (0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i+seq_length]   #get sequence of charcters withing range of 100\n",
    "    seq_out = raw_text[i+seq_length]    #get next character\n",
    "    dataX.append([chars_to_int[char] for char in seq_in])    #append all the charcters in pattern using char_to_int dict as a list to the patterns list\n",
    "    dataY.append(chars_to_int[seq_out])      #append next charcter in the pattern to list\n",
    "\n",
    "n_patterns = len(dataX)   #no of patterns with 100 charcters in dequce in the text\n",
    "\n",
    "print('total patterns : ', n_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a12391e-c768-4d09-9be5-d182da56451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern :  [1, 39, 36, 43, 47, 1, 47, 49, 46, 35, 52, 34, 36, 1, 46, 52, 49, 1, 45, 36, 54, 1, 36, 33, 46, 46, 42, 50, 11, 1, 32, 45, 35, 1, 39, 46, 54, 1, 51, 46, 0, 50, 52, 33, 50, 34, 49, 40, 33, 36, 1, 51, 46, 1, 46, 52, 49, 1, 36, 44, 32, 40, 43, 1, 45, 36, 54, 50, 43, 36, 51, 51, 36, 49, 1, 51, 46, 1, 39, 36, 32, 49, 1, 32, 33, 46, 52, 51, 1, 45, 36, 54, 1, 36, 33, 46, 46, 42, 50, 13]\n",
      "next character :  0\n"
     ]
    }
   ],
   "source": [
    "#checking random pattern with next charccter\n",
    "print('pattern : ', dataX[163679])\n",
    "print('next character : ', dataY[163679])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b171704-b4e0-415d-9857-cb5b82e1768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.81034483]\n",
      "  [0.84482759]\n",
      "  [0.79310345]\n",
      "  ...\n",
      "  [0.01724138]\n",
      "  [0.79310345]\n",
      "  [0.63793103]]\n",
      "\n",
      " [[0.84482759]\n",
      "  [0.79310345]\n",
      "  [0.70689655]\n",
      "  ...\n",
      "  [0.79310345]\n",
      "  [0.63793103]\n",
      "  [0.01724138]]\n",
      "\n",
      " [[0.79310345]\n",
      "  [0.70689655]\n",
      "  [0.62068966]\n",
      "  ...\n",
      "  [0.63793103]\n",
      "  [0.01724138]\n",
      "  [0.55172414]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.87931034]\n",
      "  [0.79310345]\n",
      "  [0.01724138]\n",
      "  ...\n",
      "  [0.79310345]\n",
      "  [0.79310345]\n",
      "  [0.72413793]]\n",
      "\n",
      " [[0.79310345]\n",
      "  [0.01724138]\n",
      "  [0.67241379]\n",
      "  ...\n",
      "  [0.79310345]\n",
      "  [0.72413793]\n",
      "  [0.86206897]]\n",
      "\n",
      " [[0.01724138]\n",
      "  [0.67241379]\n",
      "  [0.62068966]\n",
      "  ...\n",
      "  [0.72413793]\n",
      "  [0.86206897]\n",
      "  [0.22413793]]]\n"
     ]
    }
   ],
   "source": [
    "#Transform the list of input sequences into the form [samples, time steps, features] that is expected by an LSTM\n",
    "# rescale the integers to the range [0,1] to make the patterns easier to learn by the LSTM network\n",
    "# uses the sigmoid activation function by default.\n",
    "\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))  #we reshape dataX in form of [samples, time steps, features]\n",
    "\n",
    "X = X/ float(n_vocab)  #rescale to 0 -1\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "464bf72e-0a20-49ec-ae04-cdc24de2e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163680, 58)\n"
     ]
    }
   ],
   "source": [
    "#Convert the output values (single characters converted to integers) into a one hot encoding.\n",
    "#one hot encoding == > 31 ==> 00000000......100000.000 => 31st intest is 1 & rest 57 is Zero\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(dataY)\n",
    "\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72ca1c49-0a6c-4df6-a2ca-a56e47b99a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "model = Sequential()\n",
    "model.add(LSTM(255, input_shape = (X.shape[1], X.shape[2]))) \n",
    "#255--> no of neurons in LSTM laayer\n",
    "#X.shape[1] --> no of time steps\n",
    "#X.shape[12] --> no of features\n",
    "model.add(Dropout(0.2)) #20% of neurons drop when training to avoid overfitting\n",
    "model.add(Dense(Y.shape[1], activation = 'softmax')) #Y.shape[1] = 58 no of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c6a3bdb-1b32-42ce-8640-6f1aafdc0987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d98cb27d-10de-4df3-b5fa-1898c91eb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define checkpoint ==> to save weight values to use to new model when creating\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.keras\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callback_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "164d1aeb-08d4-4b62-a2d3-e96c3d63cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size =128 #no of samples proccesedd befor entire set of weights are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bd17d4d4-9bf5-4fc1-a2cc-f7856fda9acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 3.0724\n",
      "Epoch 1: loss improved from inf to 2.97360, saving model to weights-improvement-01-2.9736.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 144ms/step - loss: 3.0723\n",
      "Epoch 2/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 2.8196\n",
      "Epoch 2: loss improved from 2.97360 to 2.79733, saving model to weights-improvement-02-2.7973.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 138ms/step - loss: 2.8196\n",
      "Epoch 3/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 2.7302\n",
      "Epoch 3: loss improved from 2.79733 to 2.70455, saving model to weights-improvement-03-2.7046.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 148ms/step - loss: 2.7302\n",
      "Epoch 4/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 2.6419\n",
      "Epoch 4: loss improved from 2.70455 to 2.63354, saving model to weights-improvement-04-2.6335.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 151ms/step - loss: 2.6419\n",
      "Epoch 5/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 2.5858\n",
      "Epoch 5: loss improved from 2.63354 to 2.57749, saving model to weights-improvement-05-2.5775.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 155ms/step - loss: 2.5857\n",
      "Epoch 6/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 2.5335\n",
      "Epoch 6: loss improved from 2.57749 to 2.52663, saving model to weights-improvement-06-2.5266.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 155ms/step - loss: 2.5335\n",
      "Epoch 7/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 2.4781\n",
      "Epoch 7: loss improved from 2.52663 to 2.47492, saving model to weights-improvement-07-2.4749.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 154ms/step - loss: 2.4781\n",
      "Epoch 8/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 2.4271\n",
      "Epoch 8: loss improved from 2.47492 to 2.42651, saving model to weights-improvement-08-2.4265.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 151ms/step - loss: 2.4271\n",
      "Epoch 9/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 2.3835\n",
      "Epoch 9: loss improved from 2.42651 to 2.38316, saving model to weights-improvement-09-2.3832.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 154ms/step - loss: 2.3835\n",
      "Epoch 10/10\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 2.3441\n",
      "Epoch 10: loss improved from 2.38316 to 2.34307, saving model to weights-improvement-10-2.3431.keras\n",
      "\u001b[1m1279/1279\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 155ms/step - loss: 2.3441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x218896e6d50>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#moedl fit\n",
    "model.fit(X, Y, epochs = epochs, batch_size=batch_size, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d2509f0-b091-485c-b0f7-b760d7a25a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Text with the trained LSTM model\n",
    "filename = \"C:\\\\Users\\\\damin\\\\Downloads\\\\Practical Materials - Lab 6\\\\Practical Materials - Lab 6\"\n",
    "model.save_weights(os.path.join(filename, \"weights-improvement-10-2.3431.weights.h5\"))\n",
    "model.compile(loss='categorical_cressentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8edc67d-0594-491a-be15-fe667c011754",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1d4df537-59bc-4421-b8c8-5181c6c29823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118134\n"
     ]
    }
   ],
   "source": [
    "#genarate random seed sequence\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "148de669-d795-48fb-a4ae-409d005c737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 1, 50, 39, 36, 1, 37, 40, 49, 50, 51, 0, 50, 32, 54, 1, 51, 39, 36, 1, 54, 39, 40, 51, 36, 1, 49, 32, 33, 33, 40, 51, 13, 1, 50, 39, 36, 1, 54, 32, 50, 1, 32, 1, 43, 40, 51, 51, 43, 36, 1, 45, 36, 49, 53, 46, 52, 50, 1, 32, 33, 46, 52, 51, 1, 40, 51, 1, 41, 52, 50, 51, 1, 32, 51, 1, 37, 40, 49, 50, 51, 11, 0, 51, 39, 36, 1, 51, 54, 46, 1, 34, 49, 36, 32, 51, 52, 49, 36, 50]\n"
     ]
    }
   ],
   "source": [
    "#pattern of the random list position\n",
    "pattern = dataX[start]\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4320ed23-9371-4bf2-9098-37e325ed1fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" n she first\n",
      "saw the white rabbit. she was a little nervous about it just at first,\n",
      "the two creatures \"\n"
     ]
    }
   ],
   "source": [
    "#turn int to characters\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f0dc8c89-e26f-44ed-9339-51146972c265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#genarate characteres\n",
    "length = 0 #no of charcters want to genrate\n",
    "final = []\n",
    "for i in range(length):\n",
    "    #reshape sequce before sending to LSTM\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    #normalize values\n",
    "    x = x/float(n_vocab)\n",
    "    #predictions\n",
    "    predictions = model.predict(x, verbose =0)\n",
    "    index = np.argmax(predictions)\n",
    "    result = int_to_char[index]\n",
    "    final.append(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd621a43-9272-461b-af90-bfa847f5485c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cab34b-7da0-4762-9b66-0bd4724c6da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
